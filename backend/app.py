from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware # NEW: Import CORS
from pydantic import BaseModel
import google.generativeai as genai
from dotenv import load_dotenv
import os

load_dotenv()

# --- Gemini Configuration ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY not found in environment variables.")
genai.configure(api_key=GEMINI_API_KEY)
# --------------------------

app = FastAPI()

# --- CORS Configuration ---
# Allow requests from your frontend development server (e.g., Live Server)
origins = [
    "http://127.0.0.1:5500",  # Common Live Server port
    "http://localhost:5500",
    "*" # Use this ONLY for local development, restrict it for deployment!
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# --------------------------

# --- Pydantic Models ---
class InterviewRequest(BaseModel):
    role: str

class ResponseRequest(BaseModel):
    role: str
    user_answer: str
    history: list[str]
# --------------------------

# Placeholder for initial questions (These will be generated by Gemini now)
# START_QUESTIONS is no longer strictly needed but kept as an example.
START_QUESTIONS = {
    "software_engineer": "Tell me about yourself.",
    "data_analyst": "Why did you choose data analytics?",
    "sales": "What motivates you to sell?",
    "retail": "How would you handle a difficult customer?",
}

@app.post("/start")
async def start_interview(req: InterviewRequest):
    # Use Gemini to generate a more personalized starting question
    role = req.role
    initial_prompt = f"You are a professional interviewer for a {role} role. Start the interview with a welcoming, standard first question. Return ONLY the question with no preamble."
    
    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content(initial_prompt)
        question = response.text.strip()
    except Exception as e:
        print(f"Gemini error during start: {e}")
        question = START_QUESTIONS.get(role, "Tell me about yourself.") # Fallback
        
    # In a real app, you might also generate a session_id here.
    return {
        "question": question
    }

@app.post("/next")
async def process_answer(req: ResponseRequest):
    role = req.role
    user_answer = req.user_answer
    
    # HISTORY: history contains the list of all previous Q&A turns. 
    # The last element in history is the previous question asked by the bot.
    # We append the *current* user answer to the history here for the prompt.
    history_string = "\n".join(req.history)
    
    prompt = f"""
    You are a professional, objective interviewer for the role: {role}.
    Your goal is to conduct a realistic interview, asking follow-up questions when necessary.
    
    **CURRENT TURN:**
    The candidate's last response was: "{user_answer}"
    
    **FULL CONVERSATION HISTORY (Q&A):**
    {history_string}
    
    **YOUR TASK:**
    1. Analyze the candidate's last response.
    2. If the answer is vague, general, or lacks concrete examples, ask a specific follow-up question.
    3. If the answer is complete, transition to the next logical, standard interview question for the {role} role.
    4. After 5 turns (5 questions from you), end the interview by saying "Thank you for your time. I will now compile your feedback."
    
    Return ONLY the next question/statement with no explanation or preamble.
    """

    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = await model.generate_content_async(prompt) # Use async call
        response_text = response.text.strip()
    except Exception as e:
        print(f"Gemini error during next turn: {e}")
        response_text = "I'm sorry, I'm having trouble connecting right now. Could you rephrase your answer?" # Fallback

    return { "question": response_text }

# IMPORTANT: Ensure your GEMINI_API_KEY is set in a .env file for this to work.